{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pa\n",
    "import numpy as np \n",
    "import scipy\n",
    "pa.options.mode.chained_assignment = None\n",
    "import sys  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "w = WordPunctTokenizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "from nltk import stem\n",
    "english_stemmer = stem.SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas to Load Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pa.read_csv(\"./data/train_data.csv\", sep='\\t', header=0)\n",
    "test = pa.read_csv(\"./data/xtest_data.csv\", sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are many NaN values in the dataset, so instead of leaving them blank I filled them with Zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.fillna(\"0\")\n",
    "test = test.fillna(\"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a binary label for the label column which we have to predict and then dropping it from the training data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ytrain = train[\"type\"].map({\"ELECTRICAL\":1}).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.drop([\"type\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 1: By eye balling the data set I noticed that the type ELECTRICAL is directly associated with the license type ELECTRICAL CONTRACTOR LICENSE so I created a feature here I gave 1 eight to al columns containing this value and rest 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[\"f1\"] = 0\n",
    "train[\"f1\"][train[\"licensetype\"] == \"ELECTRICAL CONTRACTOR LICENSE\"] = 1\n",
    "\n",
    "test[\"f1\"] = 0\n",
    "test[\"f1\"][test[\"licensetype\"] == \"ELECTRICAL CONTRACTOR LICENSE\"] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 2 and 3: There is a preprocessing step for the below features for which the code is contained in extract_electrical_words.py. This file extracts unique words related with the 'type' ELECTRICAL for columns 'licensetype', 'businessname',  'description' and 'legaldescription'. This only gives us the words that are in these columns and are related with only electrical and no other type. The below code loads thos words into separate lists. There are no unique words for 'legaldescription' and 'licensetype' so we are not loading it.\n",
    "\n",
    "#### Then we create features for 'description' and 'businessname' and assign a value 1 if the row contains any of those electrical words otherwise it is given a 0 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descWords = [line.strip() for line in open(\"./data/ele_w_desc.txt\", 'r')]\n",
    "# descPattern = '|'.join(descWords).lower()\n",
    "busWords = [line.strip() for line in open(\"./data/ele_w_bus.txt\", 'r')]\n",
    "# busPattern = '|'.join(busWords).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkDesWords(tempstr, cList):\n",
    "    wlist = [word.lower() for word in w.tokenize(tempstr) if (word.isalpha() and word not in stopwords)]\n",
    "    if any(word in wlist for word in cList):\n",
    "        return True\n",
    "    \n",
    "train[\"f2\"] = 0\n",
    "train[\"f2\"] = train[\"description\"].map(lambda x: 1 if checkDesWords(x, descWords) else 0)\n",
    "# train[\"f2\"][train[\"description\"].str.contains(descPattern)] = 1\n",
    "\n",
    "train[\"f3\"] = 0\n",
    "train[\"f3\"] = train[\"businessname\"].map(lambda x: 1 if checkDesWords(x, busWords) else 0)\n",
    "\n",
    "train.job_value  = train.job_value.str.strip().str.lower().str.replace('$', '').str.replace(',','').astype(float)\n",
    "\n",
    "test[\"f2\"] = 0\n",
    "test[\"f2\"] = test[\"description\"].map(lambda x: 1 if checkDesWords(x, descWords) else 0)\n",
    "# train[\"f2\"][train[\"description\"].str.contains(descPattern)] = 1\n",
    "\n",
    "test[\"f3\"] = 0\n",
    "test[\"f3\"] = test[\"businessname\"].map(lambda x: 1 if checkDesWords(x, busWords) else 0)\n",
    "\n",
    "test.job_value  = test.job_value.str.strip().str.lower().str.replace('$', '').str.replace(',','').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram extraction and Jaccard and DIce Distance metric code. We use them to extract more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#######################\n",
    "## N-Gram Extraction ##\n",
    "#######################\n",
    "def getUnigram(words):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "        Output: a list of unigram\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "    \n",
    "def getBigram(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "       Output: a list of bigram, e.g., ['I_am', 'am_Denny']\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    if type(words) == tuple:\n",
    "        words = words[1]\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "            for k in range(1,skip+2):\n",
    "                if i+k < L:\n",
    "                    lst.append( join_string.join([words[i], words[i+k]]) )\n",
    "    else:\n",
    "        # set it as unigram\n",
    "        lst = getUnigram(words)\n",
    "    return lst\n",
    "    \n",
    "def getTrigram(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "        Output: a list of trigram, e.g., ['I_am_Denny']\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in range(L-2):\n",
    "            for k1 in range(1,skip+2):\n",
    "                for k2 in range(1,skip+2):\n",
    "                    if i+k1 < L and i+k1+k2 < L:\n",
    "                        lst.append( join_string.join([words[i], words[i+k1], words[i+k1+k2]]) )\n",
    "    else:\n",
    "        # set it as bigram\n",
    "        lst = getBigram(words, join_string, skip)\n",
    "    return lst\n",
    "    \n",
    "\n",
    "#####################\n",
    "## Distance metric ##\n",
    "#####################\n",
    "def JaccardCoef(A, B):\n",
    "    A, B = set(A), set(B)\n",
    "    intersect = len(A.intersection(B))\n",
    "    union = len(A.union(B))\n",
    "    coef = try_divide(intersect, union)\n",
    "    return coef\n",
    "\n",
    "def DiceDist(A, B):\n",
    "    A, B = set(A), set(B)\n",
    "    intersect = len(A.intersection(B))\n",
    "    union = len(A) + len(B)\n",
    "    d = try_divide(2*intersect, union)\n",
    "    return d\n",
    "\n",
    "def compute_dist(A, B, dist=\"jaccard_coef\"):\n",
    "    if dist == \"jaccard_coef\":\n",
    "        d = JaccardCoef(A, B)\n",
    "    elif dist == \"dice_dist\":\n",
    "        d = DiceDist(A, B)\n",
    "    return d\n",
    "\n",
    "def try_divide(x, y, val=0.0):\n",
    "    \"\"\" \n",
    "    \tTry to divide two numbers\n",
    "    \"\"\"\n",
    "    if y != 0.0:\n",
    "    \tval = float(x) / y\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods for Tokenizing and Stemming data and also removal of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for token in tokens:\n",
    "        stemmed.append(stemmer.stem(token))\n",
    "    return stemmed\n",
    "\n",
    "def preprocess_data(line):\n",
    "    tokens = [word.lower() for word in w.tokenize(line) if word.isalpha()]\n",
    "    tokens_stemmed = stem_tokens(tokens, english_stemmer)\n",
    "    tokens_stemmed = [x for x in tokens_stemmed if x not in stopwords]\n",
    "    return tokens_stemmed\n",
    "\n",
    "def preprocess(line):\n",
    "    return \" \".join(preprocess_data(line))\n",
    "\n",
    "def get_position_list(target, obs):\n",
    "    \"\"\"\n",
    "        Get the list of positions of obs in target\n",
    "    \"\"\"\n",
    "    pos_of_obs_in_target = [0]\n",
    "    if len(obs) != 0:\n",
    "        pos_of_obs_in_target = [j for j,w in enumerate(obs, start=1) if w in target]\n",
    "        if len(pos_of_obs_in_target) == 0:\n",
    "            pos_of_obs_in_target = [0]\n",
    "    return pos_of_obs_in_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We apply the below feature engineering for on the columns description and legal description. The below Function extracts counting features using bigrams.  Count of n-gram count, Count & Ratio of Digit count & ratio of digits in description and legal description, Count & Ratio of Unique n-gram count & ratio of unique ngram for two of the columns.\n",
    "####Description Missing Indicator binary indicator indicating whether di is empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_count_feat(df):\n",
    "    ## unigram\n",
    "    print \"generate unigram\"\n",
    "    df[\"description_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"description\"]), axis=1))\n",
    "    df[\"legaldescription_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"legaldescription\"]), axis=1))\n",
    "    ## bigram\n",
    "    print \"generate bigram\"\n",
    "    join_str = \"_\"\n",
    "    try:\n",
    "        df[\"description_bigram\"] = list(df.apply(lambda x: getBigram(x[\"description_unigram\"], join_str), axis=1))\n",
    "    except:\n",
    "        templist = []\n",
    "        for x in df[\"description_unigram\"].iteritems():\n",
    "            templist.append(getBigram(x, join_str))\n",
    "        df[\"description_bigram\"] = templist\n",
    "    \n",
    "    df[\"legaldescription_bigram\"] = list(df.apply(lambda x: getBigram(x[\"legaldescription_unigram\"], join_str), axis=1))\n",
    "    ## trigram\n",
    "    print \"generate trigram\"\n",
    "    join_str = \"_\"\n",
    "    df[\"description_trigram\"] = list(df.apply(lambda x: getTrigram(x[\"description_unigram\"], join_str), axis=1))\n",
    "    df[\"legaldescription_trigram\"] = list(df.apply(lambda x: getTrigram(x[\"legaldescription_unigram\"], join_str), axis=1))\n",
    "   \n",
    "\n",
    "    ################################\n",
    "    ## word count and digit count ##\n",
    "    ################################\n",
    "    print \"generate word counting features\"\n",
    "    feat_names = [\"description\", \"legaldescription\"]\n",
    "    grams = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "    count_digit = lambda x: sum([1. for w in x if w.isdigit()])\n",
    "    for feat_name in feat_names:\n",
    "        for gram in grams:\n",
    "            ## word count\n",
    "            df[\"count_of_%s_%s\"%(feat_name,gram)] = list(df.apply(lambda x: len(x[feat_name+\"_\"+gram]), axis=1))\n",
    "            df[\"count_of_unique_%s_%s\"%(feat_name,gram)] = list(df.apply(lambda x: len(set(x[str(feat_name+\"_\"+gram)])), axis=1))\n",
    "            df[\"ratio_of_unique_%s_%s\"%(feat_name,gram)] = map(try_divide, df[\"count_of_unique_%s_%s\"%(feat_name,gram)], df[\"count_of_%s_%s\"%(feat_name,gram)])\n",
    "\n",
    "        ## digit count\n",
    "        df[\"count_of_digit_in_%s\"%feat_name] = list(df.apply(lambda x: count_digit(x[feat_name+\"_unigram\"]), axis=1))\n",
    "        df[\"ratio_of_digit_in_%s\"%feat_name] = map(try_divide, df[\"count_of_digit_in_%s\"%feat_name], df[\"count_of_%s_unigram\"%(feat_name)])\n",
    "\n",
    "    ## description missing indicator\n",
    "    df[\"description_missing\"] = list(df.apply(lambda x: int(x[\"description_unigram\"] == \"\"), axis=1))\n",
    "    df[\"legaldescription_missing\"] = list(df.apply(lambda x: int(x[\"legaldescription_unigram\"] == \"\"), axis=1))\n",
    "\n",
    "\n",
    "    ##############################\n",
    "    ## intersect word count ##\n",
    "    ##############################\n",
    "    \n",
    "    #### Count & Ratio of a’s n-gram in b’s n-gram \n",
    "    print \"generate intersect word counting features\"\n",
    "    #### unigram\n",
    "    for gram in grams:\n",
    "        for obs_name in feat_names:\n",
    "            for target_name in feat_names:\n",
    "                if target_name != obs_name:\n",
    "                    ## query\n",
    "                    df[\"count_of_%s_%s_in_%s\"%(obs_name,gram,target_name)] = list(df.apply(lambda x: sum([1. for w in x[obs_name+\"_\"+gram] if w in set(x[target_name+\"_\"+gram])]), axis=1))\n",
    "                    df[\"ratio_of_%s_%s_in_%s\"%(obs_name,gram,target_name)] = map(try_divide, df[\"count_of_%s_%s_in_%s\"%(obs_name,gram,target_name)], df[\"count_of_%s_%s\"%(obs_name,gram)])\n",
    "\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    ## intersect word position feat ##\n",
    "    ######################################\n",
    "    #### Statistics of Positions of a’s n-gram in b’s n-gram \n",
    "    #### For those intersect n-gram, I recorded their positions, and computed the following statistics as features. \n",
    "    #     – minimum value (0% quantile) \n",
    "    #     – median value (50% quantile) \n",
    "    #     – maximum value (100% quantile) \n",
    "    #     – mean value \n",
    "    #     – standard deviation (std) \n",
    "    #### Statistics of Normalized Positions of a’s n-gram in b’s n-gram \n",
    "    #### These features are similar with above features, but computed using positions normalized by the length of a.\n",
    "\n",
    "    print \"generate intersect word position features\"\n",
    "    for gram in grams:\n",
    "        for target_name in feat_names:\n",
    "            for obs_name in feat_names:\n",
    "                if target_name != obs_name:\n",
    "                    pos = list(df.apply(lambda x: get_position_list(x[target_name+\"_\"+gram], obs=x[obs_name+\"_\"+gram]), axis=1))\n",
    "                    ## stats feat on pos\n",
    "                    df[\"pos_of_%s_%s_in_%s_min\" % (obs_name, gram, target_name)] = map(np.min, pos)\n",
    "                    df[\"pos_of_%s_%s_in_%s_mean\" % (obs_name, gram, target_name)] = map(np.mean, pos)\n",
    "                    df[\"pos_of_%s_%s_in_%s_median\" % (obs_name, gram, target_name)] = map(np.median, pos)\n",
    "                    df[\"pos_of_%s_%s_in_%s_max\" % (obs_name, gram, target_name)] = map(np.max, pos)\n",
    "                    df[\"pos_of_%s_%s_in_%s_std\" % (obs_name, gram, target_name)] = map(np.std, pos)\n",
    "                    ## stats feat on normalized_pos\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_min\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_min\" % (obs_name, gram, target_name)], df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_mean\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_mean\" % (obs_name, gram, target_name)], df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_median\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_median\" % (obs_name, gram, target_name)], df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_max\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_max\" % (obs_name, gram, target_name)], df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_std\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_std\" % (obs_name, gram, target_name)] , df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "\n",
    "# Jaccard coefficient\n",
    "# JaccardCoef(A,B) = |A∩B|/|A∪B|\n",
    "\n",
    "# and Dice distance\n",
    "# DiceDist(A,B) = 2|A∩B|/|A|+|B|\n",
    "\n",
    "# are used as distance metrics, where A and B denote two sets respectively. \n",
    "# For each distance metric, two types of features are computed. \n",
    "def extract_basic_distance_feat(df):\n",
    "    ## jaccard coef/dice dist of n-gram\n",
    "    print \"generate jaccard coef and dice dist for n-gram\"\n",
    "    dists = [\"jaccard_coef\", \"dice_dist\"]\n",
    "    grams = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "    feat_names = [\"description\", \"legaldescription\"]\n",
    "    for dist in dists:\n",
    "        for gram in grams:\n",
    "            for i in range(len(feat_names)-1):\n",
    "                for j in range(i+1,len(feat_names)):\n",
    "                    target_name = feat_names[i]\n",
    "                    obs_name = feat_names[j]\n",
    "                    df[\"%s_of_%s_between_%s_%s\"%(dist,gram,target_name,obs_name)] = \\\n",
    "                            list(df.apply(lambda x: compute_dist(x[target_name+\"_\"+gram], x[obs_name+\"_\"+gram], dist), axis=1))\n",
    "\n",
    "# The below function created TF-IDF matrix for the columns description and legal description. This may also be called \n",
    "# Bag of Words feature extraction.\n",
    "                        \n",
    "def vectorize(train, tfv_query=None):\n",
    "    #TF-IDF Calculation \n",
    "    desc_data = list(train['description'].apply(preprocess))\n",
    "    legdesc_data = list(train['legaldescription'].apply(preprocess))\n",
    "    if tfv_query is None:\n",
    "        tfv_query = TfidfVectorizer(min_df=3,  max_features=None,   \n",
    "                strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                stop_words = stopwords)\n",
    "\n",
    "        full_data = desc_data + legdesc_data\n",
    "        tfv_query.fit(full_data)   \n",
    "    \n",
    "    nonnumeric_columns = ['licensetype', 'businessname',  'subtype', 'description_unigram', 'legaldescription_unigram', 'legaldescription', 'description',\n",
    "                'description_bigram', 'legaldescription_bigram', 'description_trigram', 'legaldescription_trigram']\n",
    "    \n",
    "    # XGBoost(discussed below) doesn't (yet) handle categorical features automatically, so we need to change\n",
    "    # them to columns of integer values.\n",
    "    # See http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing for more\n",
    "    # details and options\n",
    "    le = LabelEncoder()\n",
    "    for feature in nonnumeric_columns:\n",
    "        train[feature] = le.fit_transform(train[feature])\n",
    "#     train = train.drop(drop_col, axis=1)\n",
    "    csr_train = sparse.csr_matrix(train.values)\n",
    "#     Hstack all features along with tfidf features.\n",
    "    return sparse.hstack([tfv_query.transform(desc_data), tfv_query.transform(legdesc_data), csr_train]), tfv_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Training and test features using the above methods. Xtrain is our final training matrix and ytrain is the label being predicted. Xtest is our final test matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_count_feat(train)\n",
    "extract_basic_distance_feat(train)\n",
    "\n",
    "extract_count_feat(test)\n",
    "extract_basic_distance_feat(test)\n",
    "\n",
    "Xtrain, tfv_query = vectorize(train)\n",
    "Xtest, _ = vectorize(test, tfv_query)\n",
    "\n",
    "xgb_model = XGBClassifier()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use xgboost(https://github.com/dmlc/xgboost) library created by Distributed (Deep) Machine Learning Community. It is an an optimized general purpose gradient boosting library. The library is parallelized, and also provides an optimized distributed version. It implements machine learning algorithms under the Gradient Boosting framework, including Generalized Linear Model (GLM) and Gradient Boosted Decision Trees (GBDT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# We use GridSearchCV from sklearn library to find the best parameter for the model. It takes a lot of time to predict \n",
    "# so after testing we got an accuracy of 99.4079% for tree size(max_depth) = 6,\n",
    "# no of estimators = 500 and learning_rate =0.25.\n",
    "\n",
    "#We have commented this code because it was for testing purpose and takes lot of time to process. If you want \n",
    "# to check the results this below part can be un-commented and checked again.\n",
    "\n",
    "xgb_model = XGBClassifier() \n",
    "clf = GridSearchCV(xgb_model, \n",
    "                   {'max_depth': [6], \n",
    "                    'n_estimators': [500], #tried with 50, 100, 1000 as well but best 2 parameters are 200 and 500\n",
    "                    'learning_rate': [0.25]}, verbose=1)\n",
    "clf.fit(Xtrain,ytrain)\n",
    "print clf.best_score_\n",
    "print clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The below code is another way of coing cross validation using xgboost's predevined cv funtion and we do a 10-fold\n",
    "#  cross validation and the results will be given below.\n",
    " \n",
    "dm = xgb.DMatrix(Xtrain, label=ytrain)\n",
    "params = {'max_depth': 6, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic'}\n",
    "cv = xgb.cv(params, dm, num_boost_round=10, nfold=5, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Final prediction model using the best parameters and predicting upon the test data\n",
    "\n",
    "xgb_ = XGBClassifier(max_depth=6, learning_rate=0.25, n_estimators = 500).fit(Xtrain, ytrain)\n",
    "ytest_pred = xgb_.predict(Xtest) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"./data/ytest_pred.csv\", ytest_pred, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
